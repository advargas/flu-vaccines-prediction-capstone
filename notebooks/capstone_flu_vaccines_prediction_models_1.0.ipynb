{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81b4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    " ##Dataset final\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c64453",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_features=pd.read_csv(\"training_set_features_eda_notnulls.csv\")\n",
    "y_training_features=pd.read_csv(\"training_set_labels.csv\")\n",
    "#Los id no los necesitaremos al entrenar el modelo\n",
    "x_training_features.drop([\"Unnamed: 0.1\",\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "\n",
    "#Tomando en cuenta los features que están más correlacionados, se eliminarán behavioral_outside_home\n",
    "# y doctor_recc_h1n1, tomando en cuenta tambien que tienen una menor correlación con las variables \n",
    "#targets frente a los features con los que se correlacionan.\n",
    "x_training_features.drop([\"behavioral_outside_home\",\"doctor_recc_h1n1\"], axis=1, inplace=True)\n",
    "\n",
    "#Ahora solo necesitamos de las variables targets eliminar la columna que no necesitamos: los id\n",
    "y_training_features.drop(\"respondent_id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4098cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "l_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "list_labels_encoders = [\"age_group\",\"education\"]\n",
    "list_dummies=[\"race\",\"sex\",\"marital_status\",\"rent_or_own\",\"employment_status\",\"census_msa\"]\n",
    "\n",
    "# Se aplica label encoder a las variables ordinales\n",
    "x_training_features[\"age_group\"]=l_encoder.fit_transform(x_training_features[\"age_group\"]) \n",
    "x_training_features[\"education\"]=l_encoder.fit_transform(x_training_features[\"education\"]) \n",
    "\n",
    "#se aplica one hot enocoder usando pandas: \"Dummies\"\n",
    "x_training_features=pd.get_dummies(x_training_features, columns=list_dummies, prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979d27a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['h1n1_concern', 'h1n1_knowledge', 'behavioral_antiviral_meds',\n",
       "       'behavioral_avoidance', 'behavioral_face_mask', 'behavioral_wash_hands',\n",
       "       'behavioral_large_gatherings', 'behavioral_touch_face',\n",
       "       'doctor_recc_seasonal', 'chronic_med_condition', 'child_under_6_months',\n",
       "       'health_worker', 'health_insurance', 'opinion_h1n1_vacc_effective',\n",
       "       'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc',\n",
       "       'opinion_seas_vacc_effective', 'opinion_seas_risk',\n",
       "       'opinion_seas_sick_from_vacc', 'age_group', 'education',\n",
       "       'household_adults', 'household_children', 'race_Black', 'race_Hispanic',\n",
       "       'race_White', 'sex_Female', 'marital_status_Married',\n",
       "       'rent_or_own_Rent', 'employment_status_Employed',\n",
       "       'employment_status_Not in Labor Force',\n",
       "       'census_msa_MSA, Not Principle  City',\n",
       "       'census_msa_MSA, Principle City'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#para evitar colinealidad debido a los dummies, se eliminará 1 columna por cada variable\n",
    "#las columnas generadas por one hot encoder \"dummies\" que se eliminarán son:\n",
    "list_dummies_drop=['race_Other or Multiple','sex_Male', \\\n",
    "            'marital_status_Not Married','rent_or_own_Own','employment_status_Unemployed', \\\n",
    "            'census_msa_Non-MSA']\n",
    "x_training_features.drop(list_dummies_drop,axis=1, inplace=True)\n",
    "x_training_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d23f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1n1_concern   , Cantidad de outliers:  0\n",
      "h1n1_knowledge   , Cantidad de outliers:  0\n",
      "behavioral_antiviral_meds   , Cantidad de outliers:  1301\n",
      "behavioral_avoidance   , Cantidad de outliers:  0\n",
      "behavioral_face_mask   , Cantidad de outliers:  1841\n",
      "behavioral_wash_hands   , Cantidad de outliers:  4650\n",
      "behavioral_large_gatherings   , Cantidad de outliers:  0\n",
      "behavioral_touch_face   , Cantidad de outliers:  0\n",
      "doctor_recc_seasonal   , Cantidad de outliers:  0\n",
      "chronic_med_condition   , Cantidad de outliers:  0\n",
      "child_under_6_months   , Cantidad de outliers:  2138\n",
      "health_worker   , Cantidad de outliers:  2899\n",
      "health_insurance   , Cantidad de outliers:  0\n",
      "opinion_h1n1_vacc_effective   , Cantidad de outliers:  0\n",
      "opinion_h1n1_risk   , Cantidad de outliers:  0\n",
      "opinion_h1n1_sick_from_vacc   , Cantidad de outliers:  0\n",
      "opinion_seas_vacc_effective   , Cantidad de outliers:  3427\n",
      "opinion_seas_risk   , Cantidad de outliers:  0\n",
      "opinion_seas_sick_from_vacc   , Cantidad de outliers:  6573\n",
      "age_group   , Cantidad de outliers:  0\n",
      "education   , Cantidad de outliers:  0\n",
      "household_adults   , Cantidad de outliers:  1125\n",
      "household_children   , Cantidad de outliers:  1747\n",
      "race_Black   , Cantidad de outliers:  2118\n",
      "race_Hispanic   , Cantidad de outliers:  1755\n",
      "race_White   , Cantidad de outliers:  5485\n",
      "sex_Female   , Cantidad de outliers:  0\n",
      "marital_status_Married   , Cantidad de outliers:  0\n",
      "rent_or_own_Rent   , Cantidad de outliers:  5929\n",
      "employment_status_Employed   , Cantidad de outliers:  0\n",
      "employment_status_Not in Labor Force   , Cantidad de outliers:  0\n",
      "census_msa_MSA, Not Principle  City   , Cantidad de outliers:  0\n",
      "census_msa_MSA, Principle City   , Cantidad de outliers:  0\n"
     ]
    }
   ],
   "source": [
    "#Analizando presencia de valores atípicos\n",
    "def find_outliers(df):\n",
    "    outliers = {}\n",
    "    for columna in df.columns:\n",
    "        # Calcular el primer y tercer cuartil\n",
    "        Q1 = df[columna].quantile(0.25)\n",
    "        Q3 = df[columna].quantile(0.75)\n",
    "        # Calcular el rango intercuartílico (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # Definir los límites para detectar outliers\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "        # Encontrar los índices de los outliers\n",
    "        outliers[columna] = df[(df[columna] < limite_inferior) | (df[columna] > limite_superior)].index.tolist()\n",
    "        print(columna, \"  , Cantidad de outliers: \", len(outliers[columna]))\n",
    "    return outliers\n",
    "\n",
    "    \n",
    "outliers_per_feature = find_outliers(x_training_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa20c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1n1_concern   , Cantidad de outliers:  0\n",
      "h1n1_knowledge   , Cantidad de outliers:  0\n",
      "behavioral_antiviral_meds   , Cantidad de outliers:  803\n",
      "behavioral_avoidance   , Cantidad de outliers:  0\n",
      "behavioral_face_mask   , Cantidad de outliers:  1161\n",
      "behavioral_wash_hands   , Cantidad de outliers:  3825\n",
      "behavioral_large_gatherings   , Cantidad de outliers:  0\n",
      "behavioral_touch_face   , Cantidad de outliers:  0\n",
      "doctor_recc_seasonal   , Cantidad de outliers:  0\n",
      "chronic_med_condition   , Cantidad de outliers:  0\n",
      "child_under_6_months   , Cantidad de outliers:  1487\n",
      "health_worker   , Cantidad de outliers:  2199\n",
      "health_insurance   , Cantidad de outliers:  0\n",
      "opinion_h1n1_vacc_effective   , Cantidad de outliers:  0\n",
      "opinion_h1n1_risk   , Cantidad de outliers:  0\n",
      "opinion_h1n1_sick_from_vacc   , Cantidad de outliers:  3808\n",
      "opinion_seas_vacc_effective   , Cantidad de outliers:  2521\n",
      "opinion_seas_risk   , Cantidad de outliers:  0\n",
      "opinion_seas_sick_from_vacc   , Cantidad de outliers:  0\n",
      "age_group   , Cantidad de outliers:  0\n",
      "education   , Cantidad de outliers:  0\n",
      "household_adults   , Cantidad de outliers:  807\n",
      "household_children   , Cantidad de outliers:  1237\n",
      "race_Black   , Cantidad de outliers:  1421\n",
      "race_Hispanic   , Cantidad de outliers:  1020\n",
      "race_White   , Cantidad de outliers:  3607\n",
      "sex_Female   , Cantidad de outliers:  0\n",
      "marital_status_Married   , Cantidad de outliers:  0\n",
      "rent_or_own_Rent   , Cantidad de outliers:  4109\n",
      "employment_status_Employed   , Cantidad de outliers:  0\n",
      "employment_status_Not in Labor Force   , Cantidad de outliers:  0\n",
      "census_msa_MSA, Not Principle  City   , Cantidad de outliers:  0\n",
      "census_msa_MSA, Principle City   , Cantidad de outliers:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se realizarán dos dataframes, uno con los outliers y el otro sin ellos\n",
    "#eliminando valores atipicos de la variable con más atipicos: opinion_seas_sick_from_vacc\n",
    "#se espera posteriormente ver la posibilidad de mejora en el score de lo modelos al disminuir\n",
    "#la cantidad de outliers\n",
    "x_training_features_no_outliers = \\\n",
    "x_training_features.drop(outliers_per_feature[\"opinion_seas_sick_from_vacc\"],axis=0)\n",
    "find_outliers(x_training_features_no_outliers)\n",
    "x_training_features_no_outliers.shape #20134 es la longitud\n",
    "\n",
    "#los índices donde estaban los valores atípicos se utilizan para eliminarlos también \n",
    "#en el dataframe de la variable target\n",
    "y_training_features_no_outliers = \\\n",
    "y_training_features.drop(outliers_per_feature[\"opinion_seas_sick_from_vacc\"],axis=0)\n",
    "\n",
    "#verificando que los dataframes tienen el mismo tamaño\n",
    "x_training_features_no_outliers.shape[0] == y_training_features_no_outliers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a7b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df con menos outliers:  (20134, 33)\n",
      "df con outliers:  (26707, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"df con menos outliers: \",x_training_features_no_outliers.shape)\n",
    "print(\"df con outliers: \",x_training_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f865d3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[    0     1     2 ... 26703 26704 26705]\n",
      "  Test:  index=[    4     8    12 ... 26696 26701 26706]\n",
      "Fold 1:\n",
      "  Train: index=[    0     1     4 ... 26704 26705 26706]\n",
      "  Test:  index=[    2     3     5 ... 26699 26700 26702]\n",
      "Fold 2:\n",
      "  Train: index=[    2     3     4 ... 26702 26705 26706]\n",
      "  Test:  index=[    0     1     9 ... 26693 26703 26704]\n",
      "Fold 3:\n",
      "  Train: index=[    0     1     2 ... 26704 26705 26706]\n",
      "  Test:  index=[    6    11    13 ... 26691 26695 26697]\n",
      "Fold 4:\n",
      "  Train: index=[    0     1     2 ... 26703 26704 26706]\n",
      "  Test:  index=[    7    14    23 ... 26681 26692 26705]\n"
     ]
    }
   ],
   "source": [
    "########################separando el train y test dataframe###############################\n",
    "# ESTO SOLO SE HACE A MODO EJEMPLO, YA QUE LUEGO STRATIFIEDKFOLD SE APLICA DE MANERA CONJUNTA\n",
    "#CON GRIDSERACH CV, ESTO SOLO PARA PRÓPOSITOS DE VERIFICAR SI ayuda a garantizar que cada fold \n",
    "#tenga una representación equitativa de todas las clases, evitando así la posibilidad de que un \n",
    "#fold tenga muy pocos ejemplos de una clase en particular.\n",
    "\n",
    "\n",
    "#Ya que tenemos dos variables targets, haremos este proceso para cada una\n",
    "y1=np.array(y_training_features['h1n1_vaccine'])\n",
    "y2=np.array(y_training_features['seasonal_vaccine'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "############################## h1n1_vaccine ######################################\n",
    "#splitting into 5 folds\n",
    "skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\n",
    "train_list_index, test_list_index=[],[]\n",
    "for i,(train_index, test_index) in enumerate(skf.split(x_training_features, y1)):\n",
    " print(f\"Fold {i}:\")\n",
    " print(f\"  Train: index={train_index}\")\n",
    " print(f\"  Test:  index={test_index}\")\n",
    " train_list_index.append(train_index) #saving the indexes od the 5 folds\n",
    " test_list_index.append(test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7673d2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[    0     1     2 ... 26702 26705 26706]\n",
      "  Test:  index=[    5     9    14 ... 26700 26703 26704]\n",
      "Fold 1:\n",
      "  Train: index=[    0     1     3 ... 26704 26705 26706]\n",
      "  Test:  index=[    2    12    13 ... 26685 26694 26698]\n",
      "Fold 2:\n",
      "  Train: index=[    0     2     3 ... 26704 26705 26706]\n",
      "  Test:  index=[    1     8    10 ... 26695 26699 26702]\n",
      "Fold 3:\n",
      "  Train: index=[    0     1     2 ... 26702 26703 26704]\n",
      "  Test:  index=[    4    15    29 ... 26690 26705 26706]\n",
      "Fold 4:\n",
      "  Train: index=[    1     2     4 ... 26704 26705 26706]\n",
      "  Test:  index=[    0     3     6 ... 26687 26696 26701]\n"
     ]
    }
   ],
   "source": [
    "############################## seasonal_vaccine ######################################\n",
    "#splitting into 5 folds\n",
    "skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\n",
    "train_list_index_2, test_list_index_2=[],[]\n",
    "for i,(train_index, test_index) in enumerate(skf.split(x_training_features, y2)):\n",
    " print(f\"Fold {i}:\")\n",
    " print(f\"  Train: index={train_index}\")\n",
    " print(f\"  Test:  index={test_index}\")\n",
    " train_list_index_2.append(train_index) #saving the indexes od the 5 folds\n",
    " test_list_index_2.append(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ced4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando Standard Scaler to the 5 folds\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "################################# h1n1_vaccine #######################################\n",
    "x_train,x_test,y_train,y_test=[],[],[],[]\n",
    "sc=StandardScaler()\n",
    "\n",
    "for i in range(0, len(train_list_index)):\n",
    " x_train.append(sc.fit_transform(x_training_features.iloc[train_list_index[i]]))\n",
    " x_test.append(sc.transform(x_training_features.iloc[test_list_index[i]]))\n",
    " #target variable without standard scaler\n",
    " y_train.append(y_training_features.iloc[train_list_index[i]])\n",
    " y_test.append(y_training_features.iloc[test_list_index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b98d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando Standard Scaler to the 5 folds\n",
    "################################# seasonal_vaccine #######################################\n",
    "x_train_2,x_test_2,y_train_2,y_test_2=[],[],[],[]\n",
    "sc=StandardScaler()\n",
    "\n",
    "for i in range(0, len(train_list_index)):\n",
    " x_train_2.append(sc.fit_transform(x_training_features.iloc[train_list_index_2[i]]))\n",
    " x_test_2.append(sc.transform(x_training_features.iloc[test_list_index_2[i]]))\n",
    " #target variable without standard scaler\n",
    " y_train_2.append(y_training_features.iloc[train_list_index_2[i]])\n",
    " y_test_2.append(y_training_features.iloc[test_list_index_2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c39e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "################INICIO DE DEFINICIÓN DE PARÁMETROS DE LOS MODELOS CON GRID SERCH CV#####################\n",
    "#se harán dos modelos, uno para cada variable target\n",
    "\n",
    "############################# y1=H1N1 variable target################################\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy import logspace\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Definir tus datos x_train y y_train\n",
    "\n",
    "lambdas = logspace(-11, -2, 10)\n",
    "C_ = [0.01, 0.1, 1, 10]  # How many errors can admit SVM\n",
    "gamma_ = [0.01, 0.1, 1, 10]  # How curved the decision boundaries will be\n",
    "alpha_ = [0.01, 0.1, 1, 10]  # Naive Bayes\n",
    "\n",
    "# Explanation of alpha hyperparameter:\n",
    "# Consider W* word not present in the training set\n",
    "# P(W*|Y=1) = P(W*,Y=1)/P(Y=1)\n",
    "#  = Number of training points such that w* word present and Y=1 / Number of training points where Y=1\n",
    "#  = 0/Number of training points where Y=1\n",
    "# So to get rid of this problem we do Laplace smoothing. we add alpha to numerator and denominator field.\n",
    "# = 0 + alpha / Number of training points where Y=1 + (Number of class labels in cl\n",
    "\n",
    "estimators = [\n",
    "    ('ridge', RidgeClassifierCV()),\n",
    "    ('tree', DecisionTreeClassifier(splitter=\"best\")),\n",
    "    ('svm', SVC()),\n",
    "    ('nv', BernoulliNB())\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"ridge\": {\"ridge__alphas\": lambdas},\n",
    "    \"tree\": {\"tree__max_depth\": [5, 7, 10], \"tree__ccp_alpha\": [0.1, 0.25, 0.5], \"tree__max_features\": [10, 15, 20]},\n",
    "    \"svm\": {\"svm__C\": C_, \"svm__gamma\": gamma_, \"svm__kernel\": [\"rbf\", \"linear\"]},\n",
    "    \"nv\": {\"nv__alpha\": alpha_}\n",
    "}\n",
    "\n",
    "best_params = {}\n",
    "best_score = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1234)\n",
    "\n",
    "for est_name, est in estimators:\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        (est_name, est)\n",
    "    ])\n",
    "    grid = GridSearchCV(pipeline, param_grid=params[est_name], scoring=\"roc_auc\", cv=skf, verbose=4, n_jobs=-1)\n",
    "    grid.fit(x_training_features, y1)\n",
    "    best_params[est_name] = grid.best_params_\n",
    "    best_score.append(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4c8b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the best model\n",
    "lead_board = pd.DataFrame({'model':list(best_params.keys()),\n",
    "                           'params':list(best_params.values()),\n",
    "                           'score':best_score})\n",
    "l_board =lead_board.sort_values(by=\"score\", ascending=False)\n",
    "#l_board.to_csv(\"lead_board_h1n1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87e9a69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>{'svm__C': 0.1, 'svm__gamma': 0.01, 'svm__kern...</td>\n",
       "      <td>0.823433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ridge</td>\n",
       "      <td>{'ridge__alphas': 0.001}</td>\n",
       "      <td>0.821022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nv</td>\n",
       "      <td>{'nv__alpha': 1}</td>\n",
       "      <td>0.786553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tree</td>\n",
       "      <td>{'tree__ccp_alpha': 0.1, 'tree__max_depth': 5,...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model                                             params     score\n",
       "2    svm  {'svm__C': 0.1, 'svm__gamma': 0.01, 'svm__kern...  0.823433\n",
       "0  ridge                           {'ridge__alphas': 0.001}  0.821022\n",
       "3     nv                                   {'nv__alpha': 1}  0.786553\n",
       "1   tree  {'tree__ccp_alpha': 0.1, 'tree__max_depth': 5,...  0.500000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca486e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "############################# y2=seasonal variable target################################\n",
    "\n",
    "\n",
    "lambdas = logspace(-11, -2, 10)\n",
    "C_ = [0.01, 0.1, 1, 10]  # How many errors can admit SVM\n",
    "gamma_ = [0.01, 0.1, 1, 10]  # How curved the decision boundaries will be\n",
    "alpha_ = [0.01, 0.1, 1, 10]  # Naive Bayes\n",
    "\n",
    "# Explanation of alpha hyperparameter:\n",
    "# Consider W* word not present in the training set\n",
    "# P(W*|Y=1) = P(W*,Y=1)/P(Y=1)\n",
    "#  = Number of training points such that w* word present and Y=1 / Number of training points where Y=1\n",
    "#  = 0/Number of training points where Y=1\n",
    "# So to get rid of this problem we do Laplace smoothing. we add alpha to numerator and denominator field.\n",
    "# = 0 + alpha / Number of training points where Y=1 + (Number of class labels in cl\n",
    "\n",
    "estimators = [\n",
    "    ('ridge', RidgeClassifierCV()),\n",
    "    ('tree', DecisionTreeClassifier(splitter=\"best\")),\n",
    "    ('svm', SVC()),\n",
    "    ('nv', BernoulliNB())\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"ridge\": {\"ridge__alphas\": lambdas},\n",
    "    \"tree\": {\"tree__max_depth\": [5, 7, 10], \"tree__ccp_alpha\": [0.1, 0.25, 0.5], \"tree__max_features\": [10, 15, 20]},\n",
    "    \"svm\": {\"svm__C\": C_, \"svm__gamma\": gamma_, \"svm__kernel\": [\"rbf\", \"linear\"]},\n",
    "    \"nv\": {\"nv__alpha\": alpha_}\n",
    "}\n",
    "\n",
    "best_params = {}\n",
    "best_score = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1234)\n",
    "\n",
    "for est_name, est in estimators:\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        (est_name, est)\n",
    "    ])\n",
    "    grid = GridSearchCV(pipeline, param_grid=params[est_name], scoring=\"roc_auc\", cv=skf, verbose=4, n_jobs=-1)\n",
    "    grid.fit(x_training_features, y2)\n",
    "    best_params[est_name] = grid.best_params_\n",
    "    best_score.append(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bed6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the best model\n",
    "lead_board_2 = pd.DataFrame({'model':list(best_params.keys()),\n",
    "                           'params':list(best_params.values()),\n",
    "                           'score':best_score})\n",
    "l_board_2 =lead_board_2.sort_values(by=\"score\", ascending=False)\n",
    "#l_board_2.to_csv(\"lead_board_seasonal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d2de5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>{'svm__C': 1, 'svm__gamma': 0.01, 'svm__kernel...</td>\n",
       "      <td>0.854143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ridge</td>\n",
       "      <td>{'ridge__alphas': 1e-08}</td>\n",
       "      <td>0.847609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nv</td>\n",
       "      <td>{'nv__alpha': 0.01}</td>\n",
       "      <td>0.818433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tree</td>\n",
       "      <td>{'tree__ccp_alpha': 0.1, 'tree__max_depth': 5,...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model                                             params     score\n",
       "2    svm  {'svm__C': 1, 'svm__gamma': 0.01, 'svm__kernel...  0.854143\n",
       "0  ridge                           {'ridge__alphas': 1e-08}  0.847609\n",
       "3     nv                                {'nv__alpha': 0.01}  0.818433\n",
       "1   tree  {'tree__ccp_alpha': 0.1, 'tree__max_depth': 5,...  0.500000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_board_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b2c409e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm__C': 1, 'svm__gamma': 0.01, 'svm__kernel': 'rbf'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=lead_board_2.loc[2]\n",
    "a[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bb162c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "############################# y1=H1N1 variable target################################\n",
    "###APLICANDO DE NUEVO GRID SEARCH CV, PERO AHORA SOLO PARA AFINAR (SOLO VARIANDO \n",
    "#LOs HIPERPARÁMETROS DE REGRESIÓN).\n",
    "#Y VERIFICAR SI SE PUEDE APLICAR EL MISMO MODELO PARA AMBAS VARIABLES TARGETS\n",
    "\n",
    "C_ = [0.1]  # How many errors can admit SVM\n",
    "gamma_ = [0.001]  # How curved the decision boundaries will be\n",
    "lambdas = [0.001, 1e-08]\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('ridge', RidgeClassifierCV()),\n",
    "    ('svm', SVC())\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"ridge\": {\"ridge__alphas\": lambdas},\n",
    "    \"svm\": {\"svm__C\": C_, \"svm__gamma\": gamma_, \"svm__kernel\": [\"rbf\"]}\n",
    "}\n",
    "\n",
    "best_params = {}\n",
    "best_score = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "for est_name, est in estimators:\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        (est_name, est)\n",
    "    ])\n",
    "    grid = GridSearchCV(pipeline, param_grid=params[est_name], scoring=\"roc_auc\", cv=skf, verbose=4, n_jobs=-1)\n",
    "    grid.fit(x_training_features, y1)\n",
    "    best_params[est_name] = grid.best_params_\n",
    "    best_score.append(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8628027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the best model\n",
    "lead_board_3 = pd.DataFrame({'model':list(best_params.keys()),\n",
    "                           'params':list(best_params.values()),\n",
    "                           'score':best_score})\n",
    "l_board_3 =lead_board_3.sort_values(by=\"score\", ascending=False)\n",
    "#l_board_3.to_csv(\"svm_board_h1n1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98e271ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ridge</td>\n",
       "      <td>{'ridge__alphas': 0.001}</td>\n",
       "      <td>0.847579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>svm</td>\n",
       "      <td>{'svm__C': 0.1, 'svm__gamma': 0.001, 'svm__ker...</td>\n",
       "      <td>0.844966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model                                             params     score\n",
       "0  ridge                           {'ridge__alphas': 0.001}  0.847579\n",
       "1    svm  {'svm__C': 0.1, 'svm__gamma': 0.001, 'svm__ker...  0.844966"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_board_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "83dc974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "############################# y2=seasonal variable target################################\n",
    "###APLICANDO DE NUEVO GRID SEARCH CV, PERO AHORA SOLO PARA AFINAR (SOLO VARIANDO \n",
    "#LOs HIPERPARÁMETROS DE REGRESIÓN).\n",
    "#Y VERIFICAR SI SE PUEDE APLICAR EL MISMO MODELO PARA AMBAS VARIABLES TARGETS\n",
    "\n",
    "\n",
    "C_ = [0.1]  # How many errors can admit SVM\n",
    "gamma_ = [0.001]  # How curved the decision boundaries will be\n",
    "lambdas = [0.001, 1e-08]\n",
    "# Explanation of alpha hyperparameter:\n",
    "# Consider W* word not present in the training set\n",
    "# P(W*|Y=1) = P(W*,Y=1)/P(Y=1)\n",
    "#  = Number of training points such that w* word present and Y=1 / Number of training points where Y=1\n",
    "#  = 0/Number of training points where Y=1\n",
    "# So to get rid of this problem we do Laplace smoothing. we add alpha to numerator and denominator field.\n",
    "# = 0 + alpha / Number of training points where Y=1 + (Number of class labels in cl\n",
    "\n",
    "estimators = [\n",
    "    ('ridge', RidgeClassifierCV()),\n",
    "    ('svm', SVC())\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"ridge\": {\"ridge__alphas\": lambdas},\n",
    "    \"svm\": {\"svm__C\": C_, \"svm__gamma\": gamma_, \"svm__kernel\": [\"rbf\"]}\n",
    "}\n",
    "\n",
    "best_params = {}\n",
    "best_score = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "for est_name, est in estimators:\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        (est_name, est)\n",
    "    ])\n",
    "    grid = GridSearchCV(pipeline, param_grid=params[est_name], scoring=\"roc_auc\", cv=skf, verbose=4, n_jobs=-1)\n",
    "    grid.fit(x_training_features, y2)\n",
    "    best_params[est_name] = grid.best_params_\n",
    "    best_score.append(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a6909aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the best model\n",
    "lead_board_4 = pd.DataFrame({'model':list(best_params.keys()),\n",
    "                           'params':list(best_params.values()),\n",
    "                           'score':best_score})\n",
    "l_board_4 =lead_board_4.sort_values(by=\"score\", ascending=False)\n",
    "#l_board_4.to_csv(\"svm_board_seasonal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72714bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>svm</td>\n",
       "      <td>{'svm__C': 0.1, 'svm__gamma': 0.001, 'svm__ker...</td>\n",
       "      <td>0.821622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ridge</td>\n",
       "      <td>{'ridge__alphas': 0.001}</td>\n",
       "      <td>0.820940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model                                             params     score\n",
       "1    svm  {'svm__C': 0.1, 'svm__gamma': 0.001, 'svm__ker...  0.821622\n",
       "0  ridge                           {'ridge__alphas': 0.001}  0.820940"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_board_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SE OBSERVA QUE PARA H1N1 VARIABLE TARGET ES MEJOR ridge\t{'ridge__alphas': 0.001}\n",
    "#SE OBSERVA QUE PARA SEASONAL VARIABLE TARGET ES MEJOR svm\t{'svm__C': 0.1, 'svm__gamma': 0.001,}\n",
    "#POR ENDE SE DETERMINA USAR DIFERENTES MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f4e83e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#######Determinando el mejor learning rate previo ha aplicar adaboost, usando h1n1 target#### \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define los valores de learning_rate a probar\n",
    "param_grid = {'clf__learning_rate': [0.1, 0.2]}\n",
    "\n",
    "# Crea un estimador AdaBoostClassifier con LogisticRegression y SVC como clasificadores base\n",
    "logistic_base = LogisticRegression(C=0.001, solver='lbfgs', max_iter=1000)\n",
    "svm_base = SVC(C=0.1, gamma=0.001, probability=True)\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=logistic_base, n_estimators=100)\n",
    "\n",
    "# Crea un pipeline con StandardScaler y el clasificador\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "# Define StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "# Realiza la búsqueda en rejilla con StratifiedKFold y roc_auc como métrica\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=stratified_kfold, scoring='roc_auc')\n",
    "grid_search.fit(x_training_features, y1)\n",
    "\n",
    "# El mejor valor de learning_rate\n",
    "best_learning_rate = grid_search.best_params_['clf__learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "776d4936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_learning_rate\n",
    "#es 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "daced6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\auror\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC scores: [0.8188670235992246, 0.7870681232977553, 0.8001378403089836, 0.7914018362691435, 0.7767416382302605]\n",
      "Mean ROC AUC: 0.7948432923410735\n"
     ]
    }
   ],
   "source": [
    "###########################Aplicación de adaboost, usando h1n1 target###############\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define tu learning_rate\n",
    "learning_rate = 0.2\n",
    "\n",
    "# Crea un estimador AdaBoostClassifier con LogisticRegression y SVC como clasificadores base\n",
    "logistic_base = LogisticRegression(C=0.001, solver='lbfgs', max_iter=1000)\n",
    "svm_base = SVC(C=0.1, gamma=0.001, probability=True)\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=logistic_base, n_estimators=100, learning_rate=learning_rate)\n",
    "\n",
    "# Crea un pipeline con StandardScaler y el clasificador\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "# Define StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "# Inicializa una lista para almacenar los resultados de ROC AUC\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Itera sobre los diferentes pliegues (folds)\n",
    "for train_index, test_index in stratified_kfold.split(x_training_features, y1):\n",
    "    x_train_fold, x_test_fold = x_training_features.iloc[train_index], x_training_features.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y1[train_index], y1[test_index]\n",
    "    \n",
    "    # Entrena el modelo\n",
    "    pipeline.fit(x_train_fold, y_train_fold)\n",
    "    \n",
    "    # Calcula las probabilidades\n",
    "    y_pred_proba = pipeline.predict_proba(x_test_fold)[:, 1]\n",
    "    \n",
    "    # Calcula y almacena el ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test_fold, y_pred_proba)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "# Imprime los resultados de ROC AUC\n",
    "print(f'ROC AUC scores: {roc_auc_scores}')\n",
    "print(f'Mean ROC AUC: {sum(roc_auc_scores) / len(roc_auc_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "47881745",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'learning_rate' for estimator Pipeline(steps=[('scaler', StandardScaler()),\n                ('clf', GradientBoostingClassifier(random_state=1234))]). Valid parameters are: ['memory', 'steps', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21600\\2281817223.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Realiza la búsqueda en rejilla con StratifiedKFold y roc_auc como métrica\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratified_kfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Obtiene el mejor modelo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    872\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    819\u001b[0m                     )\n\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         )\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1085\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    902\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mPipeline\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \"\"\"\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"steps\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m_set_params\u001b[1;34m(self, attr, **params)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# 3. Step parameters and other initialisation arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                 \u001b[0mlocal_valid_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    206\u001b[0m                     \u001b[1;34mf\"Invalid parameter {key!r} for estimator {self}. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                     \u001b[1;34mf\"Valid parameters are: {local_valid_params!r}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'learning_rate' for estimator Pipeline(steps=[('scaler', StandardScaler()),\n                ('clf', GradientBoostingClassifier(random_state=1234))]). Valid parameters are: ['memory', 'steps', 'verbose']."
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Se intenta aplicar GradientBoostingClassifier:\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Crea un estimador GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=1234)\n",
    "\n",
    "# Define los hiperparámetros a probar\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Define el pipeline solo con StandardScaler\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', gb_clf)\n",
    "])\n",
    "\n",
    "# Define StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "# Realiza la búsqueda en rejilla con StratifiedKFold y roc_auc como métrica\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=stratified_kfold, scoring='roc_auc')\n",
    "grid_search.fit(x_training_features.values, y1)\n",
    "\n",
    "# Obtiene el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtén las probabilidades (asegúrate de que x_test esté en el mismo formato que x_training_features)\n",
    "x_test_scaled = grid_search.best_estimator_.named_steps['scaler'].transform(x_test)\n",
    "y_pred_proba = best_model.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "# Calcula el ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC AUC en test: {roc_auc}')\n",
    "\n",
    "# Los mejores hiperparámetros\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Los mejores hiperparámetros son: {best_params}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d435415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################preparando el dataset a predecir##################################\n",
    "df_test_original=pd.read_csv(\"test_set_features.csv\")\n",
    "col=pd.read_csv(\"training_set_features_eda_notnulls.csv\").columns.tolist()\n",
    "col=col[2:]\n",
    "df_test_original=df_test_original[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55754c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5dedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
